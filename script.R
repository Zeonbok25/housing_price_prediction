# Load the housing price data with coords(generated by python script "join_data.py")
file.exists("D:/STDMcw/STDMcw/data/price_coords_16y.csv")
price <- read.csv("D:/STDMcw/STDMcw/data/price_coords_16y.csv", check.names = FALSE)

# Remove non-continental US regions
price <- price[!(price$NAME %in% c("Hawaii", "Alaska", "Puerto Rico", 
                                   "United States Virgin Islands", 
                                   "Commonwealth of the Northern Mariana Islands", 
                                   "Guam", "American Samoa")), ]

# Create state ID labels (e.g., "sta1", "sta2", ...)
state <- paste("sta", 1:nrow(price), sep = "")
price <- cbind(state, price)

# Extract only the price matrix (columns from the 21st to the end)
price_matrix <- data.matrix(price[, 21:ncol(price)])

# Check for missing values
sum(is.na(price_matrix))

# Fill missing values using last observation carried forward (LOCF)
library(zoo)
price_matrix <- na.locf(price_matrix)

# Recheck for missing values
sum(is.na(price_matrix))

#-----------------------------------------
# Exploratory Analysis: Non-spatio-temporal characteristics
#-----------------------------------------

# Calculate basic statistics
mu <- mean(price_matrix, na.rm = TRUE)
sdev <- sd(price_matrix, na.rm = TRUE)
median_val <- median(price_matrix, na.rm = TRUE)

# Plot histogram with mean and median lines
options(scipen = 10)
hist(price_matrix, breaks = 20, main = "Histogram of All Prices")
abline(v = mu, col = 'red', lwd = 2)
abline(v = median_val, col = 'blue', lwd = 2)

# Scatterplot matrix of longitude, latitude, and average price
pairs(~INTPTLON + INTPTLAT + rowMeans(price_matrix), data = price,
      main = "Scatterplot Matrix: Coordinates vs. Mean Price")

# 3D visualization of spatial distribution of mean housing prices
library(scatterplot3d)
library(plot3D)
library(rgl)

scatter3D(price$INTPTLAT, price$INTPTLON, rowMeans(price_matrix),
          main = "3D Scatterplot of Mean Housing Price")
plot3d(price$INTPTLAT, price$INTPTLON, rowMeans(price_matrix),
       main = "3D Interactive View")

#-----------------------------------------
# Temporal characteristics
#-----------------------------------------

# Plot mean housing price trend over time
plot(colMeans(price_matrix), xlab = "Month", ylab = "Price", type = "l", xaxt = "n")
axis(1, at = seq(1, 192, by = 12), labels = seq(2009, 2024, by = 1))

# Reshape price data for state-level time series plotting
library(reshape)
newprice <- melt(price, id.vars = c("state", "NAME", "INTPTLON", "INTPTLAT"),
                 measure.vars = 21:ncol(price))
colnames(newprice)[5:6] <- c("month", "price")

# Plot individual time series for selected states
library(lattice)
state.chosen <- c("sta1", "sta2", "sta3", "sta4", "sta5",
                  "sta6", "sta7", "sta8", "sta9", "sta10")
a <- newprice[state %in% state.chosen, ]
xyplot(price ~ month | state, data = a, type = "l",
       layout = c(5, 2),
       xlab = "Month", main = "House Prices Over Time (Selected States)")

#-----------------------------------------
# Heatmap Visualization
#-----------------------------------------

# Simple heatmap (non-standardized)
heatmap(price_matrix, Rowv = NA, Colv = NA,
        col = heat.colors(256), scale = "none", 
        margins = c(5, 3), xlab = "Month", ylab = "State ID")

# Reorder by longitude (for spatial visualization)
price_order <- price[order(price$INTPTLON, decreasing = TRUE), ]
price_ordermatrix <- data.matrix(price_order[, 20:ncol(price)])

# Heatmap with row normalization (emphasize temporal patterns per state)
heatmap(price_ordermatrix, Rowv = NA, Colv = NA,
        xlab = "Month", ylab = "State ID", 
        col = heat.colors(256), scale = "row", margins = c(3, 3),
        main = "Normalized Heatmap by State (Row-wise Standardized)")

#-----------------------------------------
# Spatial visualization: End-of-period price mapping
#-----------------------------------------

library(ggplot2)
library(OpenStreetMap)
library(raster)

# Prepare spatial data for mapping (use price as of Dec 2024)
data_last <- cbind(price[, c("NAME", "INTPTLAT", "INTPTLON")], price$"31/12/2024")
colnames(data_last)[4] <- "pricevalue"

# Project latitude and longitude to Mercator projection for map overlay
data_last[, 2:3] <- projectMercator(data_last$INTPTLAT, data_last$INTPTLON)

# Load base map and plot state-level prices
map <- openmap(c(49.38, -125), c(24.52, -66.93), type = 'osm')
autoplot.OpenStreetMap(map) +
  geom_point(data = data_last,
             aes(x = INTPTLAT, y = INTPTLON, color = pricevalue, size = pricevalue)) +
  ggtitle("Monthly Average Housing Price in US - December 2024")

# Histogram of state average prices (over time)
hist(rowMeans(price_matrix), breaks = 10,
     main = "Distribution of Mean Prices Across States")

# Classify states into high/low price groups
rowmeans <- rowMeans(price_matrix)
median_val <- median(rowmeans, na.rm = TRUE)
high_price <- which(rowmeans > median_val)

# Add group label for mapping
data_last_with_label <- cbind(data_last, "Low", stringsAsFactors = FALSE)
colnames(data_last_with_label)[4] <- "pricevalue"
colnames(data_last_with_label)[ncol(data_last_with_label)] <- "PriceGroup"
data_last_with_label[high_price, "PriceGroup"] <- "High"

# Map showing High vs. Low price states
autoplot.OpenStreetMap(map) +
  geom_point(data = data_last_with_label,
             aes(x = INTPTLAT, y = INTPTLON, color = PriceGroup, size = pricevalue)) +
  ggtitle("States Classified by Above/Below Median Price")
















#时空自相关
# Load the required libraries
#library(maptools)
library(lattice)
library(spdep)#wrong
#library(sp)
library(sf)
library(sftime)
#library(rgdal)
library(tmap)
library(ggplot2)
library(gridExtra)#wrong
library(gstat)#wrong
library(OpenStreetMap)
library(spacetime)#wrong
library(stars)
library(reshape)
us_state <- st_read(dsn="D:/STDMcw/STDMcw/data/tl_2023_us_state/tl_2023_us_state.shp") 
us_state <- us_state[!(us_state$NAME %in% c("Hawaii", "Alaska","Puerto Rico", "United States Virgin Islands","Commonwealth of the Northern Mariana Islands","Guam","American Samoa")), ]
nrow(us_state)
#us_states
st_crs(us_state) = 27700
price <- read.csv("D:/STDMcw/STDMcw/data/price_coords_16y.csv",check.names=FALSE)
#排除非本土
price <- price[!(price$NAME %in% c("Hawaii", "Alaska","Puerto Rico", "United States Virgin Islands","Commonwealth of the Northern Mariana Islands","Guam","American Samoa")), ]
price_matrix<-data.matrix(price[,20:ncol(price)])
sum(is.na(price_matrix))  # 计算 NA 的个数
#which(is.na(price_matrix), arr.ind=TRUE)
#用前一个填充na值
library(zoo)
price_matrix <- na.locf(price_matrix)
#price_matrix
sum(is.na(price_matrix))  # 计算 NA 的个数
install.packages("crayon")
library(crayon)
#创建10年平均房价的空间数据
price$price_avg <- rowMeans(price_matrix)

# 然后合并
priceshp <- merge(us_state, price, by="NAME")
tm_shape(priceshp) +
  tm_fill(col="price_avg", fill.scale = tm_scale_intervals(style="jenks"), 
          title="Average House Price (2009-2024)") + 
  tm_borders("white") + 
  tm_compass(position=c("left","top")) + 
  tm_scalebar() +
  tm_layout(legend.position = c("right", "bottom"))

# Calculate median of average prices
mean_price <- mean(priceshp$price_avg, na.rm = TRUE)

# Create a binary category variable
priceshp$price_category <- ifelse(priceshp$price_avg > mean_price, "Above Mean", "Below Mean")

# Create the map with two categories
tm_shape(priceshp) +
  tm_fill(col = "price_category", 
          fill.scale = tm_scale_categorical(values = c("blue", "red")),
          title = "Housing Prices (2009-2024)") + 
  tm_borders("white") + 
  tm_compass(position = c("left", "top")) + 
  tm_scalebar() +
  tm_layout(
    legend.title.size = 0.9,
    legend.text.size = 0.7,
    legend.position = c("right", "bottom")
  )








#=======================================
# Temporal autocorrelation analysis
#=======================================

# Calculate the US average monthly housing price across all states
UsMeanPrice <- colMeans(price_matrix[, 1:ncol(price_matrix)])

# Create lagged dataset (t vs t-1) for correlation visualization
UsLagged <- data.frame(
  month = seq(from = as.Date("2009-02-28"), to = as.Date("2025-02-28"), by = "month"),
  t = UsMeanPrice[2:length(UsMeanPrice)],
  t_minus_1 = UsMeanPrice[1:(length(UsMeanPrice) - 1)]
)

# Time trend plot of monthly average price
p1 <- ggplot(UsLagged, aes(x = month, y = t)) + 
  geom_line() +
  ggtitle("National Average Price Over Time")

# Lag plot with linear regression and correlation coefficient
p2 <- ggplot(UsLagged, aes(x = t, y = t_minus_1)) + 
  geom_point() + 
  labs(y = "Price (t-1)", x = "Price (t)") +
  geom_smooth(method = "lm") + 
  annotate("text", x = 175000, y = 375000, 
           label = paste("r =", round(cor(UsLagged$t, UsLagged$t_minus_1), 3)))

grid.arrange(p1, p2, nrow = 1)

# ACF and PACF for national average prices
acf(colMeans(price_matrix), lag.max = 50, main = "ACF - US Average Price")
acf(price_matrix[1, ], lag.max = 50, main = "ACF - WV (example state)")
acf(colMeans(matrix(price_matrix[1, ], 12)), lag.max = 50, main = "ACF - WV (Annual Avg)")
pacf(price_matrix[1, ], lag.max = 50, main = "PACF - WV")

#=======================================
# Spatial autocorrelation analysis
#=======================================

# Construct spatial weights matrix using polygon adjacency
W <- nb2listw(poly2nb(us_state))  # Queen contiguity weights

# Visualize spatial weights (optional)
# kable(listw2mat(W))

#--------------------
# Global Moran's I
#--------------------

# Calculate the average price per state
us_price_avg <- rowMeans(price_matrix)

# Perform Global Moran's I test
moran.test(x = us_price_avg, listw = W)

# Monte Carlo test for Global Moran's I (9999 permutations)
moran.mc(x = us_price_avg, listw = W, nsim = 9999)

#--------------------
# Local Moran's I
#--------------------

# Local spatial autocorrelation for each state
lm <- localmoran(x = rowMeans(price_matrix), listw = W)

# Add Local Moran's I statistic to spatial dataframe
priceshp$lm <- lm[, "Ii"]

# Plot raw Local Moran’s I values
tm_shape(priceshp) + 
  tm_polygons(col = "lm", palette = "-RdBu", style = "quantile") +
  tm_layout(title = "Local Moran's I (Raw Values)")

# Extract raw p-values
priceshp$lm_p <- lm[, "Pr(z != E(Ii))"]

# Assign significance labels (without correction)
priceshp$lm_sig <- "nonsignificant"
priceshp$lm_sig[which(priceshp$lm_p < 0.05)] <- "significant"

# Adjust p-values using Bonferroni correction
priceshp$lm_p_adj <- p.adjust(priceshp$lm_p, method = "bonferroni")

# Assign significance labels (after correction)
priceshp$lm_sig_adj <- "nonsignificant"
priceshp$lm_sig_adj[which(priceshp$lm_p_adj < 0.05)] <- "significant"

#--------------------
# Visualization of significance
#--------------------

# Unadjusted Local Moran's I significance map
map1 <- tm_shape(priceshp) +
  tm_fill(col = "lm_sig",
          title = "Local Moran's I Significance",
          palette = c("gray", "red"),
          labels = c("Not Significant", "Significant")) +
  tm_borders("white") +
  tm_layout(title = "Unadjusted Significance (p < 0.05)")

# Bonferroni-adjusted significance map
map2 <- tm_shape(priceshp) +
  tm_fill(col = "lm_sig_adj",
          title = "Bonferroni Adjusted Significance",
          palette = c("gray", "blue"),
          labels = c("Not Significant", "Significant")) +
  tm_borders("white") +
  tm_layout(title = "Bonferroni Adjusted (p < 0.05)")

# Print both maps
print(map1)
print(map2)

#--------------------
# LISA Cluster Typology (High-High, Low-Low, etc.)
#--------------------

# Standardize the average prices (Z-score)
price_z <- scale(rowMeans(price_matrix))

# Compute spatial lag of standardized prices
lag_price_z <- lag.listw(W, price_z)

# Classify spatial clusters
priceshp$lisa <- "Not Significant"
priceshp$lisa[priceshp$lm_p < 0.05 & price_z > 0 & lag_price_z > 0] <- "High-High"
priceshp$lisa[priceshp$lm_p < 0.05 & price_z < 0 & lag_price_z < 0] <- "Low-Low"
priceshp$lisa[priceshp$lm_p < 0.05 & price_z > 0 & lag_price_z < 0] <- "High-Low"
priceshp$lisa[priceshp$lm_p < 0.05 & price_z < 0 & lag_price_z > 0] <- "Low-High"

# LISA cluster visualization
lisa_map <- tm_shape(priceshp) +
  tm_fill(col = "lisa", title = "LISA Clusters",
          palette = c("white", "red", "lightblue", "lightpink", "green"),
          labels = c("Not Significant", "High-High", "Low-Low", "High-Low", "Low-High")) +
  tm_borders("white") +
  tm_layout(title = "Local Indicators of Spatial Association")

print(lisa_map)








##=======================================
## Spatio-temporal Forecasting with ARIMA/SARIMA
##=======================================

library(forecast)
library(ggplot2)
library(fpp2)
library(gridExtra)
library(zoo)

# 1. LOESS smoothing to observe the trend
time_index <- 1:length(colMeans(price_matrix))
trend_loess <- loess(colMeans(price_matrix) ~ time_index, span = 0.3)
trend_values <- predict(trend_loess)
plot(colMeans(price_matrix), type = "l", main = "LOESS Smoothing")
lines(trend_values, col = "red", lwd = 2)

# 2. Convert to time series (monthly frequency)
price_ts <- ts(colMeans(price_matrix), frequency = 12)

# 3. STL decomposition (Seasonal-Trend decomposition using Loess)
decom <- stl(price_ts, s.window = "periodic")
autoplot(decom)

# 4. Split into training and testing sets (last 12 months as test set)
price_ts <- ts(colMeans(price_matrix), frequency = 12, start = c(2009, 1))
price_ts_fit <- price_ts[1:182]   # Train (Jan 2009 – Feb 2024)
price_ts_test <- price_ts[183:194]  # Test (Mar 2024 – Feb 2025)

# 5. Visualize original and differenced time series
p1 <- autoplot(price_ts) +
  ggtitle("Original Time Series") + xlab("Year") + ylab("Average Price")
price_ts_diff <- diff(price_ts, differences = 1)
p2 <- autoplot(price_ts_diff) +
  ggtitle("Differenced Series") + xlab("Year") + ylab("Price Change")
grid.arrange(p1, p2, ncol = 1)

# 6. ACF and PACF plots
p1 <- autoplot(acf(price_ts, plot = FALSE))
p2 <- autoplot(pacf(price_ts, plot = FALSE))
p3 <- autoplot(acf(price_ts_diff, plot = FALSE)) 
p4 <- autoplot(pacf(price_ts_diff, plot = FALSE))
grid.arrange(p1, p2, p3, p4)

#=======================================
# Baseline ARIMA Models
#=======================================
fit.ar1 <- arima(price_ts_fit, order = c(2, 1, 0))  # ARIMA(2,1,0)
fit.ar2 <- arima(price_ts_fit, order = c(0, 1, 2))  # ARIMA(0,1,2)
fit.ar3 <- arima(price_ts_fit, order = c(2, 1, 1))  # ARIMA(2,1,1)
fit.ar4 <- arima(price_ts_fit, order = c(2, 1, 2))  # ARIMA(2,1,2)

# Compare model selection criteria
AIC(fit.ar1, fit.ar2, fit.ar3, fit.ar4)
BIC(fit.ar1, fit.ar2, fit.ar3, fit.ar4)

# Forecast using best ARIMA model
forecast_values <- forecast(fit.ar4, h = length(price_ts_test))
predictions <- forecast_values$mean
errors <- price_ts_test - predictions
RMSE <- sqrt(mean(errors^2, na.rm = TRUE))
MAE <- mean(abs(errors), na.rm = TRUE)

cat("Test Set RMSE: ", RMSE, "\n")
cat("Test Set MAE: ", MAE, "\n")

# Residual diagnostics
tsdiag(fit.ar4)

# Plot fitted vs actual (ARIMA)
pre.ar <- predict(fit.ar4, n.ahead = 12)
matplot(1:12, cbind(price_ts[183:194], pre.ar$pred), type = "l", xlab = "Month", ylab = "Avg Housing Price")

#=======================================
# Seasonal Components
#=======================================

# Check seasonality using ACF/PACF with seasonal differencing
acf(price_ts, lag.max = 36)
price_sdiff <- diff(price_ts, lag = 12)
acf(price_sdiff, lag.max = 48)
pacf(price_sdiff, lag.max = 48)

# Try several SARIMA model variants
fit.sar1 <- arima(price_ts_fit, order = c(2, 1, 2), seasonal = list(order = c(1, 0, 1), period = 12))
fit.sar2 <- arima(price_ts_fit, order = c(2, 1, 2), seasonal = list(order = c(0, 1, 1), period = 12))
fit.sar3 <- arima(price_ts_fit, order = c(2, 1, 2), seasonal = list(order = c(1, 1, 0), period = 12))

# Evaluate using NRMSE
NRMSE_fit_sar1 <- NRMSE(res = fit.sar1$residuals, obs = price_ts_fit)
NRMSE_fit_sar2 <- NRMSE(res = fit.sar2$residuals, obs = price_ts_fit)
NRMSE_fit_sar3 <- NRMSE(res = fit.sar3$residuals, obs = price_ts_fit)

print(NRMSE_fit_sar1)
print(NRMSE_fit_sar2)
print(NRMSE_fit_sar3)

#=======================================
# Grid Search for Best SARIMA Model
#=======================================

train <- price_ts_fit
test <- price_ts_test

# Use auto.arima to scan possible combinations
best_model <- auto.arima(train, 
                         d = 1, D = 1,
                         max.p = 3, max.q = 3,
                         max.P = 2, max.Q = 2,
                         seasonal = TRUE,
                         stepwise = FALSE,
                         approximation = FALSE,
                         trace = TRUE)

# Manually record top 5 models (by AIC)
top_models <- list(
  model1 = list(order = c(2, 1, 2), seasonal = list(order = c(0, 1, 1), period = 12)),
  model2 = list(order = c(1, 1, 3), seasonal = list(order = c(0, 1, 1), period = 12)),
  model3 = list(order = c(1, 1, 2), seasonal = list(order = c(0, 1, 1), period = 12)),
  model4 = list(order = c(1, 1, 2), seasonal = list(order = c(0, 1, 2), period = 12)),
  model5 = list(order = c(1, 1, 2), seasonal = list(order = c(1, 1, 1), period = 12))
)

# Evaluate each model by RMSE and MAE
evaluate_model <- function(model_spec, train_data, test_data) {
  model_fit <- Arima(train_data, order = model_spec$order, seasonal = model_spec$seasonal)
  forecasts <- forecast(model_fit, h = length(test_data))
  errors <- test_data - forecasts$mean
  rmse <- sqrt(mean(errors^2, na.rm = TRUE))
  mae <- mean(abs(errors), na.rm = TRUE)
  
  return(list(
    model = paste0("ARIMA(", paste(model_spec$order, collapse = ","), ")",
                   "(", paste(model_spec$seasonal$order, collapse = ","), ")",
                   model_spec$seasonal$period),
    AIC = AIC(model_fit),
    RMSE = rmse,
    MAE = mae
  ))
}

results <- lapply(top_models, function(model) {
  evaluate_model(model, train, test)
})

# Format results for display
results_df <- do.call(rbind, lapply(results, function(x) {
  data.frame(Model = x$model, AIC = x$AIC, RMSE = x$RMSE, MAE = x$MAE)
}))
results_df <- results_df[order(results_df$RMSE), ]
print("Models Ranked by RMSE:")
print(results_df)

# Final selected model: SARIMA(1,1,2)(1,1,1)[12]
fit.sarbest <- arima(price_ts_fit, 
                     order = c(1, 1, 2),
                     seasonal = list(order = c(1, 1, 1), period = 12))

NRMSE_fit_sarbest <- NRMSE(res = fit.sarbest$residuals, obs = price_ts_fit)
print(NRMSE_fit_sarbest)

# Forecast with final SARIMA
pre.sar <- predict(fit.sarbest, n.ahead = 12)
matplot(1:12, cbind(price_ts[183:194], pre.sar$pred), type = "l", 
        xlab = "Month", ylab = "Average Housing Price", main = "SARIMA Forecast")

# Evaluate forecast error on training set
RMSE_fit_sarbest <- sqrt(mean((fit.sarbest$residuals)^2, na.rm = TRUE))
MAE_fit_sarbest <- mean(abs(fit.sarbest$residuals), na.rm = TRUE)

cat("Final SARIMA RMSE: ", RMSE_fit_sarbest, "\n")
cat("Final SARIMA MAE: ", MAE_fit_sarbest, "\n")











##===============================
## Spatio-Temporal Modeling with STARIMA
##===============================

library(spdep)

# Create neighborhood structure using queen contiguity
nbrs <- poly2nb(us_state)

# Convert to spatial weights matrix (binary 0/1)
W1 <- nb2mat(nbrs)

# Generate higher-order neighbors (e.g., 2nd-order for more spatial reach)
Wlist <- nblag(nbrs, 3)
W2 <- nb2mat(Wlist[[2]])  # Second-order neighbors

# Transpose price matrix to match STARIMA format (time in rows, space in columns)
us_price_mat <- t(price_matrix)

#===========================
# Spatio-temporal autocorrelation
#===========================

# Compute STACF on original data
stacf(us_price_mat, W1, 48)

# Apply differencing to remove trend/seasonality
us_price_mat1.diff <- diff(us_price_mat, lag = 1, differences = 1)   # First-order differencing
us_price_mat12.diff <- diff(us_price_mat, lag = 12, differences = 1) # Seasonal differencing

# STACF after differencing
stacf(us_price_mat1.diff, W1, 48)
stacf(us_price_mat12.diff, W1, 48)

# STPACF analysis
stpacf(us_price_mat, W1, 48)
stpacf(us_price_mat1.diff, W1, 48)
stpacf(us_price_mat12.diff, W1, 48)

# Optional: temporal-only ACF (no spatial structure)
W0 <- diag(x = 1, nrow(W1), ncol(W1))
stacf(us_price_mat1.diff, W0, 48)
stacf(us_price_mat12.diff,W0,48)
#fit
W_fit<-list(w1=W2) # Create a list of spatial weight matrices, zero not needed
fit.star <- starima_fit(Z=us_price_mat[1:182,],W=W_fit,p=2,d=1,q=2)
stacf(fit.star$RES,W2,48)
hist(fit.star$RES[,6])
Box.test(fit.star$RES[,6],lag=1, type="Ljung")

pre.star <- starima_pre(us_price_mat[(182-1-4+1):194,],
                        model=fit.star)
matplot(1:12,cbind(us_price_mat[183:194,1],pre.star$PRE[,1]),type="l")

# Get the actual values from the test set
actual_values <- us_price_mat[183:194, ]

# The predictions are in pre.star$PRE
predicted_values <- pre.star$PRE

# Calculate errors for each location/variable
errors <- actual_values - predicted_values

# Calculate RMSE
# First, square all errors
squared_errors <- errors^2
# Take the mean of squared errors (across all locations and time points)
mean_squared_error <- mean(squared_errors, na.rm = TRUE)
# Take the square root to get RMSE
RMSE <- sqrt(mean_squared_error)

# Calculate MAE
# Take the absolute value of all errors
absolute_errors <- abs(errors)
# Take the mean of absolute errors
MAE <- mean(absolute_errors, na.rm = TRUE)

# Print results
#cat("Test set RMSE:", RMSE, "\n")
#cat("Test set MAE:", MAE, "\n")

# If you want RMSE and MAE for each location separately
location_RMSE <- sqrt(colMeans(squared_errors, na.rm = TRUE))
location_MAE <- colMeans(absolute_errors, na.rm = TRUE)

# Print location-specific metrics
cat("Average location-specific RMSE:", mean(location_RMSE), "\n")
cat("Average location-specific MAE:", mean(location_MAE), "\n")


